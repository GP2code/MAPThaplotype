{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAPT - Haplotype analyses\n",
    "\n",
    "## GP2 NBA data release 7\n",
    "\n",
    "## Project: Exploring MAPT-containing H1 and H2 haplotypes  in Parkinson's Disease across diverse populations \n",
    "\n",
    "Version: Python/3.10.12\n",
    "\n",
    "Last Updated: 29-MAY-2024\n",
    "\n",
    "Gene coordinates for the region of 17q21.31 (containing MAPT) from the UCSC Browser: chr17:42,800,001-46,800,000 (GRCh38/hg38)\n",
    "\n",
    "Notebook overview: In this notebook we performed analyses looking at the frequency of haplotypes in PD cases and controls in MAPT using the tagging SNP rs1052553. In this notebook, we specifically looked at the AAC ancestry group but the analysis was repeated on the other ancestries available in GP2 (with the exception of the FIN due to low sample size).\n",
    "\n",
    "\n",
    "1. Set up everything. \n",
    "2. Extract the rs1052553 SNP - H1/H2 haplotype \n",
    "3. Calculate HWE for the SNP\n",
    "4. Get the frequency and number of individuals of H1 vs H2 without covariates\n",
    "5. Get the frequency and number of individuals of H1/H1, H1/H2 and H2/H2 \n",
    "6. Run association analysis with covariates for H1 vs H2\n",
    "7. Testing the groups H1/H1 vs H1/H2 and H2/H2 (dominant model)\n",
    "8. Testing the three groups: H1/H1 (reference) vs H1/H2 vs H2/H2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "Loading Python libraries and defining functions\n",
    "Installing packages\n",
    "Preparing input files:\n",
    "- Copying files \n",
    "- Remove related individuals\n",
    "- Remove non-PD case control individuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Python libraries and defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the os package to interact with the environment\n",
    "import os\n",
    "\n",
    "# Bring in Pandas for Dataframe functionality\n",
    "import pandas as pd\n",
    "\n",
    "# Numpy for basics\n",
    "import numpy as np\n",
    "\n",
    "# Use StringIO for working with file contents\n",
    "from io import StringIO\n",
    "\n",
    "# Enable IPython to display matplotlib graphs\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable interaction with the FireCloud API\n",
    "from firecloud import api as fapi\n",
    "\n",
    "# Import the iPython HTML rendering for displaying links to Google Cloud Console\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# Import urllib modules for building URLs to Google Cloud Console\n",
    "import urllib.parse\n",
    "\n",
    "# BigQuery for querying data\n",
    "from google.cloud import bigquery\n",
    "\n",
    "#Import Sys\n",
    "import sys as sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility routine for printing a shell command before executing it\n",
    "def shell_do(command):\n",
    "    print(f'Executing: {command}', file=sys.stderr)\n",
    "    !$command\n",
    "    \n",
    "def shell_return(command):\n",
    "    print(f'Executing: {command}', file=sys.stderr)\n",
    "    output = !$command\n",
    "    return '\\n'.join(output)\n",
    "\n",
    "# Utility routine for printing a query before executing it\n",
    "def bq_query(query):\n",
    "    print(f'Executing: {query}', file=sys.stderr)\n",
    "    return pd.read_gbq(query, project_id=BILLING_PROJECT_ID, dialect='standard')\n",
    "\n",
    "# Utility routine for display a message and a link\n",
    "def display_html_link(description, link_text, url):\n",
    "    html = f'''\n",
    "    <p>\n",
    "    </p>\n",
    "    <p>\n",
    "    {description}\n",
    "    <a target=_blank href=\"{url}\">{link_text}</a>.\n",
    "    </p>\n",
    "    '''\n",
    "\n",
    "    display(HTML(html))\n",
    "\n",
    "# Utility routines for reading files from Google Cloud Storage\n",
    "def gcs_read_file(path):\n",
    "    \"\"\"Return the contents of a file in GCS\"\"\"\n",
    "    contents = !gsutil -u {BILLING_PROJECT_ID} cat {path}\n",
    "    return '\\n'.join(contents)\n",
    "    \n",
    "def gcs_read_csv(path, sep=None):\n",
    "    \"\"\"Return a DataFrame from the contents of a delimited file in GCS\"\"\"\n",
    "    return pd.read_csv(StringIO(gcs_read_file(path)), sep=sep, engine='python')\n",
    "\n",
    "# Utility routine for displaying a message and link to Cloud Console\n",
    "def link_to_cloud_console_gcs(description, link_text, gcs_path):\n",
    "    url = '{}?{}'.format(\n",
    "        os.path.join('https://console.cloud.google.com/storage/browser',\n",
    "                     gcs_path.replace(\"gs://\",\"\")),\n",
    "        urllib.parse.urlencode({'userProject': BILLING_PROJECT_ID}))\n",
    "\n",
    "    display_html_link(description, link_text, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up billing project and data path variables\n",
    "BILLING_PROJECT_ID = os.environ['GOOGLE_PROJECT']\n",
    "WORKSPACE_NAMESPACE = os.environ['WORKSPACE_NAMESPACE']\n",
    "WORKSPACE_NAME = os.environ['WORKSPACE_NAME']\n",
    "WORKSPACE_BUCKET = os.environ['WORKSPACE_BUCKET']\n",
    "\n",
    "WORKSPACE_ATTRIBUTES = fapi.get_workspace(WORKSPACE_NAMESPACE, WORKSPACE_NAME).json().get('workspace',{}).get('attributes',{})\n",
    "\n",
    "## Print the information to check we are in the proper release and billing \n",
    "## This will be different for you, the user, depending on the billing project your workspace is on\n",
    "print('Billing and Workspace')\n",
    "print(f'Workspace Name: {WORKSPACE_NAME}')\n",
    "print(f'Billing Project: {BILLING_PROJECT_ID}')\n",
    "print(f'Workspace Bucket, where you can upload and download data: {WORKSPACE_BUCKET}')\n",
    "print('')\n",
    "\n",
    "## GP2 v7.0\n",
    "## Explicitly define release v7.0 path \n",
    "GP2_RELEASE_PATH = 'gs://gp2tier2/path/to/release/7'\n",
    "GP2_CLINICAL_RELEASE_PATH = f'{GP2_RELEASE_PATH}/clinical_data'\n",
    "GP2_RAW_GENO_PATH = f'{GP2_RELEASE_PATH}/raw_genotypes'\n",
    "GP2_IMPUTED_GENO_PATH = f'{GP2_RELEASE_PATH}/imputed_genotypes'\n",
    "GP2_META_RELEASE_PATH = f'{GP2_RELEASE_PATH}/meta_data'\n",
    "GP2_SUMSTAT_RELEASE_PATH = f'{GP2_RELEASE_PATH}/summary_statistics'\n",
    "\n",
    "print('GP2 v7.0')\n",
    "print(f'Path to GP2 v7.0 Clinical Data @ `GP2_CLINICAL_RELEASE_PATH`: {GP2_CLINICAL_RELEASE_PATH}')\n",
    "print(f'Path to GP2 v7.0 Metadata @ `GP2_META_RELEASE_PATH`: {GP2_META_RELEASE_PATH}')\n",
    "print(f'Path to GP2 v7.0 Raw Genotype Data @ `GP2_RAW_GENO_PATH`: {GP2_RAW_GENO_PATH}')\n",
    "print(f'Path to GP2 v7.0 Imputed Genotype Data @ `GP2_IMPUTED_GENO_PATH`: {GP2_IMPUTED_GENO_PATH}')\n",
    "print(f'Path to GP2 v7.0 summary statistics: {GP2_SUMSTAT_RELEASE_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing packages and softwares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#Installing plink\n",
    "\n",
    "mkdir -p ~/tools\n",
    "cd ~/tools\n",
    "\n",
    "if test -e /home/jupyter/tools/plink; then\n",
    "echo \"Plink1.9 is already installed in /home/jupyter/tools/\"\n",
    "\n",
    "else\n",
    "echo -e \"Downloading plink \\n    -------\"\n",
    "wget -N http://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20190304.zip \n",
    "unzip -o plink_linux_x86_64_20190304.zip\n",
    "echo -e \"\\n plink downloaded and unzipped in /home/jupyter/tools \\n \"\n",
    "\n",
    "fi\n",
    "\n",
    "\n",
    "if test -e /home/jupyter/tools/plink2; then\n",
    "echo \"Plink2 is already installed in /home/jupyter/tools/\"\n",
    "\n",
    "else\n",
    "echo -e \"Downloading plink2 \\n    -------\"\n",
    "wget -N https://s3.amazonaws.com/plink2-assets/alpha6/plink2_linux_x86_64_20250129.zip\n",
    "unzip -o plink2_linux_x86_64_20250129.zip\n",
    "echo -e \"\\n plink2 downloaded and unzipped in /home/jupyter/tools \\n \"\n",
    "\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls /home/jupyter/tools/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# chmod plink 1.9 \n",
    "chmod u+x /home/jupyter/tools/plink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# chmod plink 2.0\n",
    "chmod u+x /home/jupyter/tools/plink2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a directory\n",
    "print(\"Making a working directory\")\n",
    "WORK_DIR = f'/home/jupyter/Team6_haplo/'\n",
    "shell_do(f'mkdir -p {WORK_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Retreive the files needed, including the genotype (iusing the raw genotype files) and covariate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shell_do(f'gsutil -mu {BILLING_PROJECT_ID} ls {GP2_RAW_GENO_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} -m cp -r {GP2_RAW_GENO_PATH}/AAC/AAC_* {WORK_DIR}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the covariate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} ls {GP2_CLINICAL_RELEASE_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} -m cp -r {GP2_CLINICAL_RELEASE_PATH}/master_key_release7_final.csv {WORK_DIR}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove related individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select the file that matches with your population\n",
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} ls {GP2_META_RELEASE_PATH}/related_samples/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} -m cp -r {GP2_META_RELEASE_PATH}/related_samples/AAC_release7.related {WORK_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat /home/jupyter/Team6_haplo/AAC_release7.related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IDs are:\n",
    "ID1: Individual ID for the first individual of the pair\n",
    "ID2: Individual ID for the second individual of the pair\n",
    "We select to remove individuals in the ID1 and only exclude one person in the pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "WORK_DIR='/home/jupyter/Team6_haplo/'\n",
    "cd $WORK_DIR\n",
    "\n",
    "\n",
    "cut -d, -f2 AAC_release7.related > related_ids.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!cat /home/jupyter/Team6_haplo/related_ids.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "WORK_DIR='/home/jupyter/Team6_haplo/'\n",
    "cd $WORK_DIR\n",
    "\n",
    "/home/jupyter/tools/plink2 \\\n",
    "--pfile AAC_release7 \\\n",
    "--remove related_ids.txt \\\n",
    "--make-pgen \\\n",
    "--out AAC_release7_nonrelated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove non-PD case/control individuals\n",
    "\n",
    "Double-check with the numbers found here for your ancestry group before moving on: https://gp2.org/the-components-of-gp2s-fifth-data-release/\n",
    "\n",
    "The prune flag keeo only these with a plink phenotype of 1 or 0. We need to do this because the MAF will be different if these individuals are not removed (for the group all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "WORK_DIR='/home/jupyter/Team6_haplo/'\n",
    "cd $WORK_DIR\n",
    "\n",
    "/home/jupyter/tools/plink2 \\\n",
    "--pfile AAC_release7_nonrelated \\\n",
    "--prune \\\n",
    "--make-pgen \\\n",
    "--out AAC_release7_nonrelated_pdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "WORK_DIR='/home/jupyter/Team6_haplo/'\n",
    "cd $WORK_DIR\n",
    "head AAC_release7_nonrelated_pdc.pvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the region of interest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are interested in the SNP rs1052553\n",
    "- This SNP was the one that they used in the Nigerian MAPT paper\n",
    "- This SNP will be used as a proxy for the H1/H2 haplotype\n",
    "- rs1052553 coordinates in GRCh38: 17:45996523\n",
    "- We will also add --mind to remove individuals that haven't been fully genotyped for this variant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "WORK_DIR='/home/jupyter/Team6_haplo/'\n",
    "cd $WORK_DIR\n",
    "\n",
    "/home/jupyter/tools/plink2 \\\n",
    "--pfile AAC_release7_nonrelated_pdc \\\n",
    "--chr 17 \\\n",
    "--from-bp 45996523  \\\n",
    "--to-bp 45996523 \\\n",
    "--mind \\\n",
    "--make-pgen \\\n",
    "--out haplo_h1h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "WORK_DIR='/home/jupyter/Team6_haplo/'\n",
    "cd $WORK_DIR\n",
    "head haplo_h1h2.pvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are two variants here with the same coordinates (At least for the AAC population). This is because there were multipel probes for the same variant during genotyping - the results for the variants should be indentical though"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate HWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#We will chack if the SNP deviate from HWE\n",
    "\n",
    "WORK_DIR='/home/jupyter/Team6_haplo/'\n",
    "cd $WORK_DIR\n",
    "\n",
    "/home/jupyter/tools/plink2 \\\n",
    "--pfile haplo_h1h2 \\\n",
    "--hardy \\\n",
    "--keep-if PHENO1==1 \\\n",
    "--out haplo_h1h2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "WORK_DIR='/home/jupyter/Team6_haplo/'\n",
    "cd $WORK_DIR\n",
    "head haplo_h1h2.hardy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the p-value to the HWE sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put together the covar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clin = pd.read_csv('/home/jupyter/Team6_haplo/master_key_release7_final.csv')\n",
    "clin.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen = pd.read_csv('/home/jupyter/Team6_haplo/AAC_release7.psam', sep='\\t')\n",
    "gen.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pcs = pd.read_csv('/home/jupyter/Team6_haplo/AAC_release7.eigenvec', sep='\\t')\n",
    "pcs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen2 = pd.merge(gen, clin, left_on='#IID', right_on='GP2sampleID')\n",
    "gen2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen3 = pd.merge(gen2, pcs, left_on='#IID', right_on='IID')\n",
    "gen3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plink_clin = gen3[['#IID', 'SEX', 'PHENO1', 'age_at_sample_collection', 'PC1', 'PC2', 'PC3', 'PC4', 'PC5', \"PC6\",\"PC7\",\"PC8\",\"PC9\",\"PC10\"]]\n",
    "plink_clin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Set missing values to -9 (plink format)\n",
    "plink_clin['PHENO1'] = plink_clin['PHENO1'].fillna(-9)\n",
    "plink_clin['age_at_sample_collection'] = plink_clin['age_at_sample_collection'].fillna(-9)\n",
    "plink_clin['SEX'] = plink_clin['SEX'].fillna(-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plink_clin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename age_at_sample_collection  \n",
    "plink_clin = plink_clin.rename(columns={'age_at_sample_collection': 'AGE'})\n",
    "plink_clin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plink_clin.to_csv('/home/jupyter/Team6_haplo/covars.txt', sep='\\t', index=False, na_rep='-9',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "WORK_DIR='/home/jupyter/Team6_haplo/'\n",
    "cd $WORK_DIR\n",
    "\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the frequencies for H1 and H2 in cases and controls - \"without covariates\" (also N)\n",
    "\n",
    "This includes all individuals, even the ones we are missing covariates for.\n",
    "This part have been updated in this notebook (hopefully to the better!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A) H1 vs H2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) Convert to plink1.9 binary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "WORK_DIR='/home/jupyter/Team6_haplo/'\n",
    "cd $WORK_DIR\n",
    "\n",
    "/home/jupyter/tools/plink2 \\\n",
    "--pfile haplo_h1h2  \\\n",
    "--make-bed \\\n",
    "--out haplo_h1h2_recode_bed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Run --assoc to get the freq in cases and in controls and also the p-value for potential differences in the allele frequency between cases and controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "WORK_DIR='/home/jupyter/Team6_haplo/'\n",
    "cd $WORK_DIR\n",
    "\n",
    "/home/jupyter/tools/plink \\\n",
    "--bfile haplo_h1h2_recode_bed  \\\n",
    "--assoc \\\n",
    "--ci 0.95 \\\n",
    "--out haplo_h1h2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the number of cases and controls from the plink output. Here: 509 cases and 301 controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat /home/jupyter/Team6_haplo/haplo_h1h2.assoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the numbers above to the H1 vs H2 (WITHOUT covariates, --assoc) sheet in our results document\n",
    "\n",
    "Add the frequencies and the N cases and controls to the first sheet\n",
    "F_A = Frequency affected (=PD)\n",
    "F_U = Frequency unaffected (=Controls)\n",
    "\n",
    "Below, we will also get the number of alleles (no need to add this to the sheet, not sure we need it but in case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "WORK_DIR='/home/jupyter/Team6_haplo/'\n",
    "cd $WORK_DIR\n",
    "\n",
    "/home/jupyter/tools/plink \\\n",
    "--bfile haplo_h1h2_recode_bed  \\\n",
    "--assoc counts \\\n",
    "--ci 0.95 \\\n",
    "--out haplo_h1h2_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /home/jupyter/Team6_haplo/haplo_h1h2_counts.assoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the frequencies for H1/H1, H1/H2 and H2/H2 \n",
    "\n",
    "H1/H1, H1/H2, H2/H2 groups:\n",
    "\n",
    "#H1H1 = 0\n",
    "#H1H2 = 1\n",
    "#H2H2 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "WORK_DIR='/home/jupyter/Team6_haplo/'\n",
    "cd $WORK_DIR\n",
    "\n",
    "/home/jupyter/tools/plink \\\n",
    "--bfile haplo_h1h2_recode_bed \\\n",
    "--recode A \\\n",
    "--out haplo_h1h2_recodeA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!head /home/jupyter/Team6_haplo/haplo_h1h2_recodeA.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cas_haplos = pd.read_csv('/home/jupyter/Team6_haplo/haplo_h1h2_recodeA.raw', sep=' ')\n",
    "cas_haplos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#total number of samples\n",
    "cas_haplos = pd.read_csv('/home/jupyter/Team6_haplo/haplo_h1h2_recodeA.raw', sep=' ')\n",
    "cas_haplos.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#remove haplotype that are not defined under column chr17...\n",
    "\n",
    "cas_haplos_clean = cas_haplos[~cas_haplos['rs1052553_G'].isna()]\n",
    "cas_haplos_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#select only PD cases (total N of PD cases)\n",
    "cas_haplos_case = cas_haplos_clean[cas_haplos_clean['PHENOTYPE']==2]\n",
    "cas_haplos_case.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#select only controls (Total N of controls)\n",
    "cas_haplos_control = cas_haplos_clean[cas_haplos_clean['PHENOTYPE']==1]\n",
    "cas_haplos_control.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#total no of H1H1 in both pd & ctrl (no need to write this down in the table)\n",
    "cas_h1h1 = cas_haplos_clean[cas_haplos_clean['rs1052553_G'] == 0]\n",
    "cas_h1h1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#no of H1H1 in pd (case) only\n",
    "cas_h1h1_cases = cas_haplos_case[cas_haplos_case['rs1052553_G'] == 0]\n",
    "cas_h1h1_cases.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#no of H1H1 in ctrls only\n",
    "cas_h1h1_controls = cas_haplos_control[cas_haplos_control['rs1052553_G'] == 0]\n",
    "cas_h1h1_controls.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#total no of H1H2 in both pd & ctrl (no need to write this down in the table)\n",
    "cas_h1h2 = cas_haplos_clean[cas_haplos_clean['rs1052553_G'] == 1]\n",
    "cas_h1h2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#no of H1H2 in pd (case) only\n",
    "cas_h1h2_cases = cas_haplos_case[cas_haplos_case['rs1052553_G'] == 1]\n",
    "cas_h1h2_cases.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#no of H1H2 in ctrls only\n",
    "cas_h1h2_controls = cas_haplos_control[cas_haplos_control['rs1052553_G'] == 1]\n",
    "cas_h1h2_controls.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#total no of H2H2 in both pd & ctrl (no need to write this down in the table)\n",
    "cas_h2h2 = cas_haplos_clean[cas_haplos_clean['rs1052553_G'] == 2]\n",
    "cas_h2h2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#no of H2H2 in pd (case) only\n",
    "cas_h2h2_cases = cas_haplos_case[cas_haplos_case['rs1052553_G'] == 2]\n",
    "cas_h2h2_cases.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#no of H2H2 in ctrls only\n",
    "cas_h2h2_controls = cas_haplos_control[cas_haplos_control['rs1052553_G'] == 2]\n",
    "cas_h2h2_controls.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the cas_h1h1_cases to look at the AAO (H1/H1 vs H1/H2 and H2/H2)\n",
    "cas_h1h1_cases.iloc[:, :2].to_csv('aac_h1h1_cases.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing the association between PD and H1 vs H2 haplotypes. Run association analysis with covariates\n",
    "\n",
    "Age, Sex, PC1-PC5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "WORK_DIR='/home/jupyter/Team6_haplo/'\n",
    "cd $WORK_DIR\n",
    "\n",
    "/home/jupyter/tools/plink2 \\\n",
    "--pfile haplo_h1h2 \\\n",
    "--glm hide-covar firth-fallback pheno-ids \\\n",
    "--covar-name AGE,SEX,PC1,PC2,PC3,PC4,PC5 \\\n",
    "--pheno-name PHENO1 \\\n",
    "--pheno /home/jupyter/Team6_haplo/covars.txt \\\n",
    "--ci 0.95 \\\n",
    "--covar-variance-standardize \\\n",
    "--covar /home/jupyter/Team6_haplo/covars.txt \\\n",
    "--out haplo_h1h2_glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat /home/jupyter/Team6_haplo/haplo_h1h2_glm.PHENO1.glm.logistic.hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBS_CT for the regression indicate the number of samples in the regression (not alleles as before). Here: 639 samples. (! note that it's not the same numbers when plink is loading the phenotypes sinces it loads all 509 cases and 301 controls = 810 samples). Therefore, we need to get info on how many cases and controls we have in the regression with covariates\n",
    "\n",
    "The pheno-ids addition to the --glm gives us a list of the IDs for the individuals that were kept in the analysis. We can use this to extract the cases and controls that were included in the analysis to count them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "covars = pd.read_csv('/home/jupyter/Team6_haplo/haplo_h1h2_glm.PHENO1.glm.logistic.hybrid.id', sep='\\t')\n",
    "covars.head()\n",
    "\n",
    "len(covars.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, 639 out of 810 samples (in the AAC population) were included in the regression when the covariates were added. We will check the frequency for the 639 that were included in the cells above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the frequency of the H1 and H2 haplotypes for the individuals that were included in the regression analysis with covariates:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "WORK_DIR='/home/jupyter/Team6_haplo/'\n",
    "cd $WORK_DIR\n",
    "\n",
    "/home/jupyter/tools/plink2 \\\n",
    "--pfile haplo_h1h2 \\\n",
    "--keep haplo_h1h2_glm.PHENO1.glm.logistic.hybrid.id \\\n",
    "--freq \\\n",
    "--make-pgen \\\n",
    "--out haplo_h1h2_covariates_N_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat /home/jupyter/Team6_haplo/haplo_h1h2_covariates_N_all.afreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "WORK_DIR='/home/jupyter/Team6_haplo/'\n",
    "cd $WORK_DIR\n",
    "\n",
    "/home/jupyter/tools/plink2 \\\n",
    "--pfile haplo_h1h2 \\\n",
    "--keep haplo_h1h2_glm.PHENO1.glm.logistic.hybrid.id \\\n",
    "--keep-if PHENO1='2' \\\n",
    "--freq \\\n",
    "--make-pgen \\\n",
    "--out haplo_h1h2_covariates_N_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!cat /home/jupyter/Team6_haplo/haplo_h1h2_covariates_N_cases.afreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the plink output gives us \"411 cases and 0 controls remaining after main filters.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONTROLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "WORK_DIR='/home/jupyter/Team6_haplo/'\n",
    "cd $WORK_DIR\n",
    "\n",
    "/home/jupyter/tools/plink2 \\\n",
    "--pfile haplo_h1h2 \\\n",
    "--keep haplo_h1h2_glm.PHENO1.glm.logistic.hybrid.id \\\n",
    "--keep-if PHENO1='1' \\\n",
    "--freq \\\n",
    "--make-pgen \\\n",
    "--out haplo_h1h2_covariates_N_controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!cat /home/jupyter/Team6_haplo/haplo_h1h2_covariates_N_controls.afreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there were 411 cases and 228 controls (639 samples) in the adjusted analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing H1/H1 vs H1/H2 and H2/H2 (dominant model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'dominant' modifier specifies a model assuming full dominance for the A1 allele, i.e. the first genotype column is changed to 0..1..1 encoding. Similarly, 'recessive' makes the first genotype column use 0..0..1 encoding.\n",
    "\n",
    "Hence, using the dominant modifier in plink group H1/H1 vs H1/H2 and H2/H2. We are doing this due to the few number of individuals having H2/H2 (but we are looking at all three groups a few cells down!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "WORK_DIR='/home/jupyter/Team6_haplo/'\n",
    "cd $WORK_DIR\n",
    "\n",
    "/home/jupyter/tools/plink2 \\\n",
    "--pfile haplo_h1h2 \\\n",
    "--glm dominant hide-covar firth-fallback \\\n",
    "--covar-name AGE,SEX,PC1,PC2,PC3,PC4,PC5 \\\n",
    "--pheno-name PHENO1 \\\n",
    "--pheno /home/jupyter/Team6_haplo/covars.txt \\\n",
    "--ci 0.95 \\\n",
    "--covar-variance-standardize \\\n",
    "--covar /home/jupyter/Team6_haplo/covars.txt \\\n",
    "--out haplo_h1h2_gml_dominant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat /home/jupyter/Team6_haplo/haplo_h1h2_gml_dominant.PHENO1.glm.logistic.hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the results above to the H1/H1 vs H1/H2 and H2/H2 sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the three groups: H1/H1 (reference) vs H1/H2 vs H2/H2\n",
    "#H1H1 = 0\n",
    "#H1H2 = 1\n",
    "#H2H2 = 2\n",
    "\n",
    "We are evaluating the risk of having PD if you are a H1/H2 carrier or a H2/H2 carrier as compared to if you would be a H1/H1 carrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cas_haplos = pd.read_csv('/home/jupyter/Team6_haplo/haplo_h1h2_recodeA.raw', sep=' ')\n",
    "cas_haplos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set phenotype to 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cas_haplos['PHENOTYPE'] -= 1\n",
    "cas_haplos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the haplotype groups to categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cas_haplos.rename(columns={\"rs1052553_G\": \"Haplo\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cas_haplos.info() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the covariates and update missing ages (-9) to NA. PLINK interpret -9 as a missing value but python does not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covars = pd.read_csv('/home/jupyter/Team6_haplo/covars.txt', sep='\\t')\n",
    "covars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "covars.replace(-9.0, np.nan, inplace=True)\n",
    "covars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "haplo_groups = pd.merge(cas_haplos, covars, left_on='IID', right_on='#IID')\n",
    "haplo_groups.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "haplo_log = smf.logit(formula = 'PHENOTYPE ~ C(Haplo) + SEX_x + AGE + PC1 + PC2 + PC3 + PC4 + PC5' , data = haplo_groups).fit() \n",
    "haplo_log.summary() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the odds ratio (OR) and 95% confidence interval (CI) for the OR (above is for the coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params = haplo_log.params\n",
    "conf = haplo_log.conf_int()\n",
    "conf['Odds Ratio'] = params\n",
    "conf.columns = ['2.5% CI', '97.5% CI', 'Odds Ratio']\n",
    "print(np.exp(conf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBS! An error message can occur if you have no H2/H2 haplotype carriers\n",
    "\n",
    "Please add the results to the H1/H1 vs H1/H2 vs H2/H2 sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the H1 association to PD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are flipping the minor and major allele to get the H2 haplotype to be the reference haplotype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare allele.txt file to flip major/minor alleles\n",
    "# Define the SNP and allele\n",
    "snp = 'rs1052553'\n",
    "allele = 'G'\n",
    "\n",
    "# Specify the output file name\n",
    "output_file = '/home/jupyter/Team6_haplo/alleles.txt'\n",
    "\n",
    "# Write the SNP and allele to the file\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write(f\"{snp} {allele}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "WORK_DIR='/home/jupyter/Team6_haplo/'\n",
    "cd $WORK_DIR\n",
    "\n",
    "/home/jupyter/tools/plink \\\n",
    "--bfile haplo_h1h2_recode_bed  \\\n",
    "--assoc \\\n",
    "--a2-allele alleles.txt \\\n",
    "--ci 0.95 \\\n",
    "--out flip_haplo_h1h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! cat /home/jupyter/Team6_haplo/flip_haplo_h1h2.assoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "WORK_DIR='/home/jupyter/Team6_haplo/'\n",
    "cd $WORK_DIR\n",
    "\n",
    "/home/jupyter/tools/plink2 \\\n",
    "--pfile haplo_h1h2 \\\n",
    "--ref-allele alleles.txt \\\n",
    "--make-pgen \\\n",
    "--out flipped_plink_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "WORK_DIR='/home/jupyter/Team6_haplo/'\n",
    "cd $WORK_DIR\n",
    "\n",
    "/home/jupyter/tools/plink2 \\\n",
    "--pfile flipped_plink_file \\\n",
    "--glm hide-covar omit-ref firth-fallback pheno-ids \\\n",
    "--covar-name AGE,SEX,PC1,PC2,PC3,PC4,PC5 \\\n",
    "--pheno-name PHENO1 \\\n",
    "--pheno /home/jupyter/Team6_haplo/covars.txt \\\n",
    "--ci 0.95 \\\n",
    "--covar-variance-standardize \\\n",
    "--covar /home/jupyter/Team6_haplo/covars.txt \\\n",
    "--out flip_haplo_h1h2_glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /home/jupyter/Team6_haplo/flip_haplo_h1h2_glm.PHENO1.glm.logistic.hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GWAS for locus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The region to be used corresponds to 17q21.31 chr17:42800001-46800000 according to UCSC Genome Browser on Human (GRCh38/hg38)\n",
    "\n",
    "https://genome.ucsc.edu/cgi-bin/hgTracks?db=hg38&lastVirtModeType=default&lastVirtModeExtraState=&virtModeType=default&virtMode=0&nonVirtPosition=&position=chr17%3A42800001%2D46800000&hgsid=2300497464_YMBoqmHnJWaakVS6O5MFDdk8kbGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "WORK_DIR='/home/jupyter/Team6_haplo/'\n",
    "cd $WORK_DIR\n",
    "\n",
    "/home/jupyter/tools/plink2 \\\n",
    "--pfile AAC_release7_nonrelated_pdc \\\n",
    "--chr 17 \\\n",
    "--maf 0.01 \\\n",
    "--from-bp 42800001 \\\n",
    "--to-bp 46800000 \\\n",
    "--mind \\\n",
    "--make-pgen \\\n",
    "--out AAC_locus_17q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "WORK_DIR='/home/jupyter/Team6_haplo/'\n",
    "cd $WORK_DIR\n",
    "\n",
    "/home/jupyter/tools/plink2 \\\n",
    "--pfile AAC_locus_17q \\\n",
    "--maf 0.01 \\\n",
    "--glm hide-covar --ci 0.95 \\\n",
    "--covar /home/jupyter/Team6_haplo/covars.txt \\\n",
    "--covar-name SEX,AGE,PC1,PC2,PC3,PC4,PC5,PC6,PC7,PC8,PC9,PC10 \\\n",
    "--pheno-name PHENO1 \\\n",
    "--pheno /home/jupyter/Team6_haplo/covars.txt \\\n",
    "--covar-variance-standardize \\\n",
    "--out AAC_locus_17q_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head /home/jupyter/Team6_haplo/AAC_locus_17q_out.PHENO1.glm.logistic.hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_do(f'gsutil -mu {BILLING_PROJECT_ID} cp -r /home/jupyter/Team6_haplo/AAC_locus_17q_out.PHENO1.glm.logistic.hybrid {WORKSPACE_BUCKET}/PLOT_All_pops/AAC_locus_17q.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "540.556px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
